I"¨∞<h1 id="monte-carlo-methods">Monte Carlo Methods</h1>

<p>This notebook is an implementation of Monte Carlo (MC) algorithms that is
introduced in Richard Sutton‚Äôs book applied to Blackjack environment of OpenAI‚Äôs
gym. I used Udacity‚Äôs visualisation function for ploting</p>

<p>With MC we don‚Äôt assume full knowledge of the environment. We only need
experience-sample sequences of (states, actions, reward)  from actual or
simulated interaction with an environment. This requires no prior knowledge of
the environment‚Äôs dynamics. We can approximate the model by generating samples
of transitions according to a desired probability distribution, but we can‚Äôt
obtain the distribution in explicit form as required for dynamic programming.</p>

<p>Monte Carlo methods in this context are ways of solving the reinforcement
learning problem based on averaging sample returns.</p>

<p>General Policy Iteration (GPI):</p>

<p>Policy Evaluation $\leftrightarrow$ Policy Improvement</p>

<p>Monte Carlo prediction</p>

<p>We learn the state-value function of a given policy by interacting with the
environment (collecting samples of interactions) using a that policy.</p>

<p>Policy Improvement:</p>

<p>Control</p>

<p>Advantages of MC over DP:</p>
<ul>
  <li>It doesn‚Äôt require knowledge of the environment model.</li>
  <li>Unlike DP it doesn‚Äôt rely on bootstrapping, thus, values of states are
estimated independently and we can learn values of subset of states if we always
start sampling from these states.</li>
  <li>It can learn from actual experience or simulated one?</li>
  <li>Less harmed by violations of the Markov property</li>
</ul>

<blockquote>
  <p>In control methods we are particularly interested in approximating action-
value functions,
because these can be used to improve the policy without requiring a model of the
environment‚Äôs transition dynamics</p>
</blockquote>

<p>Monte Carlo estimate of stat-action values can‚Äôt use determinstic policies as it
will never visit some trajectories, so we add an element of exploration.</p>

<p>The first-visit MC method estimates $v_{œÄ}(s)$
as the average of the returns following first visits to s</p>

<p>every-visit MC method averages
the returns following all visits to s.</p>

<p><strong>In [16]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">plot_utils</span> <span class="kn">import</span> <span class="n">plot_blackjack_values</span><span class="p">,</span> <span class="n">plot_policy</span></code></pre></figure>

<h4 id="first-visit-mc-prediction-for-estimating-v--vœÄ">First-visit MC prediction, for estimating $V ‚âà vœÄ$</h4>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Initialize:
    œÄ ‚Üê policy to be evaluated
    V ‚Üê an arbitrary state-value function
    Returns(s) ‚Üê an empty list, for all s ‚àà S

Repeat forever:
    Generate an episode using œÄ
    For each state s appearing in the episode:
        G ‚Üê the return that follows the first occurrence of s
        Append G to Returns(s)
        V (s) ‚Üê average(Returns(s))
</code></pre></div></div>

<p>Source:
<a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">Richard S. Sutton</a></p>

<h3 id="creating-an-instance-of-the-blackjackhttpsgithubcomopenaigymblobm">Creating an instance of the [Blackjack](https://github.com/openai/gym/blob/m</h3>
<p>aster/gym/envs/toy_text/blackjack.py) environment.</p>

<p><strong>In [5]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'Blackjack-v0'</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.
  result = entry_point.load(False)
</code></pre></div></div>

<p>Each state is a 3-tuple of:</p>
<ul>
  <li>the player‚Äôs current sum $\in {0, 1, \ldots, 31}$,</li>
  <li>the dealer‚Äôs face up card $\in {1, \ldots, 10}$, and</li>
  <li>whether or not the player has a usable ace (<code class="highlighter-rouge">no</code> $=0$, <code class="highlighter-rouge">yes</code> $=1$).</li>
</ul>

<p>The agent has two potential actions:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    STICK = 0
    HIT = 1
</code></pre></div></div>

<h3 id="part-1-mc-prediction-estimating-the-action-value-function">Part 1: MC Prediction: (estimating the action-value function)</h3>

<p>Definitions:</p>

<p><code class="highlighter-rouge">env</code>: This is an instance of OpenAI Gym‚Äôs Blackjack environment.
<code class="highlighter-rouge">episode</code>: This is a list of (state, action, reward) tuples (of tuples) and
corresponds to $(S_0, A_0, R_1, \ldots, S_{T-1}, A_{T-1}, R_{T})$, where $T$ is
the final time step.
<code class="highlighter-rouge">Q</code>: A dictionary (of one-dimensional arrays) where <code class="highlighter-rouge">Q[s][a]</code> is the estimated
action value corresponding to state <code class="highlighter-rouge">s</code> and action <code class="highlighter-rouge">a</code>.
<code class="highlighter-rouge">gamma</code>: The discount rate.  It must be a value between 0 and 1, inclusive
(default value: <code class="highlighter-rouge">1</code>). In this case we will select <code class="highlighter-rouge">1</code> as it is an episodic
environment and we care more about the reward in the final step.
<code class="highlighter-rouge">alpha</code>: This is the step-size parameter for the update step.</p>

<p>Will start by defining a random policy to explore the environment, then we will
evaluate the <code class="highlighter-rouge">Q</code> (state-action) values based on this policy</p>

<p><strong>In [688]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">random_policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">generate_episode_from_policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
    <span class="s">'''
    env: 
    policy:
    '''</span>
    
    <span class="n">episode</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">))</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">break</span>
            
    <span class="k">return</span> <span class="n">episode</span></code></pre></figure>

<h3 id="updating-q-values-using-incremental-mean">Updating Q values Using Incremental Mean</h3>

<p>$Q(S_{t}, A_{t}) \leftarrow Q(S_{t}, A_{t}) + \frac{1}{N(S_{t}, A_{t})}(G_{t} -
Q(S_t - A_t))$</p>

<p>We update Q value after each episode by the error between estimated return
compared to stored Q value for that state-action averaged by the number of times
we visited this state-action pair before:</p>

<p>$\delta_t = (G_{t} - Q(S_t, A_t))$</p>

<p><strong>In [716]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">mc_prediction_q</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">num_episodes</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    
    <span class="c1"># initialize empty dictionaries of arrays
</span>    <span class="n">returns_sum</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
    <span class="c1"># loop over episodes
</span>    <span class="k">for</span> <span class="n">i_episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># monitor progress
</span>        <span class="k">if</span> <span class="n">i_episode</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\r</span><span class="s">Episode {}/{}."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i_episode</span><span class="p">,</span> <span class="n">num_episodes</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s">""</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
            
        <span class="n">episode</span> <span class="o">=</span> <span class="n">generate_episode_from_policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">)</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">episode</span><span class="p">)</span>
        
        <span class="n">discount_factors</span> <span class="o">=</span> <span class="p">[</span><span class="n">gamma</span><span class="o">**</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>

        <span class="n">first_visit_flag</span> <span class="o">=</span> <span class="p">{(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)}</span>
        
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)):</span>
            
            <span class="k">if</span> <span class="n">first_visit_flag</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">)]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                
                <span class="n">discounted_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">*</span><span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">idx</span><span class="p">:],</span> <span class="n">discount_factors</span><span class="p">[:</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">idx</span><span class="p">)])]</span>
                <span class="n">sum_discounted_reward</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">discounted_rewards</span><span class="p">)</span>
                
                <span class="n">returns_sum</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">sum_discounted_reward</span>
                <span class="n">N</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">returns_sum</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">/</span> <span class="n">N</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
            
                <span class="n">first_visit_flag</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
    
        
    <span class="k">return</span> <span class="n">Q</span></code></pre></figure>

<h3 id="lets-evaluate-the-state-action-value-q-using-random-policy">Let‚Äôs evaluate the state-action value ($Q$) using random policy</h3>

<p><strong>In [None]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># obtain the action-value function
</span><span class="n">Q</span> <span class="o">=</span> <span class="n">mc_prediction_q</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="mi">50000</span><span class="p">,</span> <span class="n">random_policy</span><span class="p">)</span></code></pre></figure>

<h4 id="we-can-visualise-q-function-in-a-3d-plot">We can visualise Q function in a 3D plot</h4>

<p>Plotting the state-action value in two plots as each state consist of three
variables : Player‚Äôs Current Sum - Dealer‚Äôs Showing Card - Usable Ace</p>

<p><strong>In [717]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># obtain the corresponding state-value function
</span><span class="n">V_to_plot</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">k</span><span class="p">,(</span><span class="n">k</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">18</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span><span class="n">v</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="n">k</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">&lt;=</span><span class="mi">18</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span><span class="n">v</span><span class="p">)))</span> \
         <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">Q</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>

<span class="n">plot_blackjack_values</span><span class="p">(</span><span class="n">V_to_plot</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Episode 50000/50000.
</code></pre></div></div>

<p><img src="/images/monte_carlo_17_1.png" alt="png" /></p>

<h3 id="part-2-mc-control">Part 2: MC Control</h3>

<p>Your algorithm has four arguments:</p>
<ul>
  <li><code class="highlighter-rouge">env</code>: This is an instance of an OpenAI Gym environment.</li>
  <li><code class="highlighter-rouge">num_episodes</code>: This is the number of episodes that are generated through
agent-environment interaction.</li>
  <li><code class="highlighter-rouge">alpha</code>: This is the step-size parameter for the update step.</li>
  <li><code class="highlighter-rouge">gamma</code>: This is the discount rate.  It must be a value between 0 and 1,
inclusive (default value: <code class="highlighter-rouge">1</code>).</li>
</ul>

<p>The algorithm returns as output:</p>
<ul>
  <li><code class="highlighter-rouge">Q</code>: This is a dictionary (of one-dimensional arrays) where <code class="highlighter-rouge">Q[s][a]</code> is the
estimated action value corresponding to state <code class="highlighter-rouge">s</code> and action <code class="highlighter-rouge">a</code>.</li>
  <li><code class="highlighter-rouge">policy</code>: This is a dictionary where <code class="highlighter-rouge">policy[s]</code> returns the action that the
agent chooses after observing state <code class="highlighter-rouge">s</code>.</li>
</ul>

<p>$œÄ(s).= arg max_{a}q(s, a)$</p>

<p><strong>In [6]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">generate_episode_from_Q</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">):</span>
    
    
    <span class="n">episode</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    
    
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        
        <span class="k">if</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">Q</span><span class="p">:</span>
        
            <span class="c1">### initialise probabilites of all actions to be epsilon / n_actions
</span>            <span class="n">probability_of_random_action</span> <span class="o">=</span> <span class="n">epsilon</span> <span class="o">/</span><span class="n">n_actions</span>
            <span class="n">probability_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_actions</span><span class="p">)</span> <span class="o">*</span> <span class="n">probability_of_random_action</span>
            <span class="n">best_action_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
            <span class="n">probability_vector</span><span class="p">[</span><span class="n">best_action_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span> <span class="o">+</span> <span class="n">probability_of_random_action</span>
            
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_actions</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">probability_vector</span><span class="p">)</span>
            
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            
            
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">episode</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">))</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">break</span>
            
    <span class="k">return</span> <span class="n">episode</span>       </code></pre></figure>

<p><strong>In [7]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">evaluate_Q</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">n_episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    
    <span class="n">status_map</span> <span class="o">=</span> <span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="s">'loss'</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="s">'draw'</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="s">'win'</span><span class="p">}</span>

    <span class="n">average_reward</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>

        <span class="n">average_reward</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generate_episode_from_Q</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_actions</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="p">{</span><span class="n">status_map</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span><span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">Counter</span><span class="p">(</span><span class="n">average_reward</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span></code></pre></figure>

<h4 id="mc-control-constant-alpha">MC-Control constant-alpha</h4>

<p>We would like to give more weights to returns estimated recently more than ones
estimated at the first few episodes as we expect the recent ones to be generated
from a better policy. Thus, we replace the weighting average with a constant
alpha that determines how much we want to emphasize later returns and how much
we forget from the past</p>

<p>$Q(S_{t}, A_{t}) \leftarrow Q(S_{t}, A_{t}) + \alpha (G_{t} - Q(S_t, A_t))$</p>

<p>We can re-write this equation as:</p>

<p>$Q(S_t,A_t) \leftarrow (1-\alpha)Q(S_t,A_t) + \alpha G_t$</p>

<ul>
  <li>If $\alpha=0$, then the action-value function estimate is never updated by the
agent.</li>
  <li>If $\alpha = 1$, then the final value estimate for each state-action pair is
always equal to the last return that was experienced by the agent (after
visiting the pair).</li>
</ul>

<p><strong>In [11]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">update_Q</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">):</span>


    
    <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">episode</span><span class="p">)</span>

    <span class="n">returns_sum</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_actions</span><span class="p">))</span>


    <span class="n">discount_factors</span> <span class="o">=</span> <span class="p">[</span><span class="n">gamma</span><span class="o">**</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>

    <span class="n">first_visit_flag</span> <span class="o">=</span> <span class="p">{(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)}</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)):</span>

        <span class="k">if</span> <span class="n">first_visit_flag</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">)]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            
            <span class="n">discounted_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">*</span><span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">idx</span><span class="p">:],</span> <span class="n">discount_factors</span><span class="p">[:</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">idx</span><span class="p">)])]</span>
            <span class="n">sum_discounted_reward</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">discounted_rewards</span><span class="p">)</span>
            
            <span class="n">Q_old</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">sum_discounted_reward</span> <span class="o">-</span>  <span class="n">Q_old</span>
            <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q_old</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">delta</span>

            <span class="n">first_visit_flag</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>    

    <span class="k">return</span> <span class="n">Q</span></code></pre></figure>

<p><strong>In [9]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">mc_control</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">num_episodes</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon_start</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
               <span class="n">epsilon_decay</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">epsilon_min</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_start</span>
    <span class="n">n_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="c1"># initialize empty dictionary of arrays
</span>    <span class="n">Q</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_actions</span><span class="p">))</span>
    <span class="c1"># loop over episodes
</span>    <span class="k">for</span> <span class="n">i_episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># monitor progress
</span>        <span class="k">if</span> <span class="n">i_episode</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\r</span><span class="s">Episode {}/{}."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i_episode</span><span class="p">,</span> <span class="n">num_episodes</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s">""</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
        
        <span class="c1">## updating epsilon and policy before collecting new episode
</span>        <span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">epsilon</span> <span class="o">*</span> <span class="n">epsilon_decay</span><span class="p">,</span> <span class="n">epsilon_min</span><span class="p">)</span>
        
        
        
        <span class="n">episode</span> <span class="o">=</span> <span class="n">generate_episode_from_Q</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>
        
        <span class="n">Q</span> <span class="o">=</span> <span class="n">update_Q</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>
    
    <span class="n">policy</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">Q</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    
    <span class="k">return</span> <span class="n">policy</span><span class="p">,</span> <span class="n">Q</span></code></pre></figure>

<p><strong>In [32]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># obtain the estimated optimal policy and action-value function
</span><span class="n">policy</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">mc_control</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                       <span class="n">epsilon_start</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">epsilon_decay</span><span class="o">=</span><span class="mf">0.99999</span><span class="p">,</span> <span class="n">epsilon_min</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Episode 1000000/1000000.
</code></pre></div></div>

<p><strong>In [33]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">evaluate_Q</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">Q</span><span class="p">)</span></code></pre></figure>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'draw': 81, 'win': 413, 'loss': 506}
</code></pre></div></div>

<h3 id="plotting-the-corresponding-state-value-function">Plotting the corresponding state-value function.</h3>

<p><strong>In [34]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># obtain the corresponding state-value function
</span><span class="n">V</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">k</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">v</span><span class="p">))</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">Q</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>

<span class="c1"># plot the state-value function
</span><span class="n">plot_blackjack_values</span><span class="p">(</span><span class="n">V</span><span class="p">)</span></code></pre></figure>

<p><img src="/images/monte_carlo_28_0.png" alt="png" /></p>

<p>Finally, we visualize the policy that is estimated to be optimal.</p>

<p>Epsilon greedy.</p>

<p>$
\pi(a|s) \longleftarrow
\begin{cases}
\displaystyle 1-\epsilon +\epsilon/|\mathcal{A}(s)|&amp;amp; \textrm{if
}a\textrm{ maximizes }Q(s,a)<br />
\displaystyle \epsilon/|\mathcal{A}(s)| &amp;amp; \textrm{else}
$</p>

<p>$\pi(a|s) \longleftarrow
\displaystyle 1-\epsilon +\epsilon/|\mathcal{A}(s)| \textrm{if }a\textrm{
maximizes }Q(s,a) <br />
\epsilon \mathcal{A}(s)|  \textrm{else} $</p>

<p><strong>In [35]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># plot the policy
</span><span class="n">plot_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span></code></pre></figure>

<p><img src="/images/monte_carlo_32_0.png" alt="png" /></p>

<p>The <strong>true</strong> optimal policy $\pi_*$ can be found in Figure 5.2 of the
<a href="http://go.udacity.com/rl-textbook">textbook</a> (and appears below).
<img src="images/optimal.png" alt="True Optimal Policy" /></p>

<p><strong>In [206]:</strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="err">!</span><span class="nb">open</span> <span class="nb">file</span><span class="p">:</span><span class="o">///</span><span class="n">Users</span><span class="o">/</span><span class="n">amr</span><span class="o">/</span><span class="n">Downloads</span><span class="o">/</span><span class="n">nd893</span><span class="o">/</span><span class="n">Deep</span><span class="o">%</span><span class="mi">20</span><span class="n">Reinforcement</span><span class="o">%</span><span class="il">20L</span><span class="n">earning</span><span class="o">%</span><span class="mi">20</span><span class="n">Nanodegree</span><span class="o">%</span><span class="mi">20</span><span class="n">v2</span><span class="mf">.0.0</span><span class="o">/</span><span class="n">Part</span><span class="o">%</span><span class="mi">2001</span><span class="o">-</span><span class="n">Module</span><span class="o">%</span><span class="mi">2001</span><span class="o">-</span><span class="n">Lesson</span><span class="o">%</span><span class="mi">2008</span><span class="n">_Monte</span><span class="o">%</span><span class="mi">20</span><span class="n">Carlo</span><span class="o">%</span><span class="mi">20</span><span class="n">Methods</span><span class="o">/</span><span class="n">img</span><span class="o">/</span><span class="n">screen</span><span class="o">-</span><span class="n">shot</span><span class="o">-</span><span class="mi">2018</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="n">at</span><span class="o">-</span><span class="mf">2.49.48</span><span class="o">-</span><span class="n">pm</span><span class="o">.</span><span class="n">png</span></code></pre></figure>

<p><img src="file:///Users/amr/Downloads/nd893/Deep%20Reinforcement%20Learning%20Nano
degree%20v2.0.0/Part%2001-Module%2001-Lesson%2008_Monte%20Carlo%20Methods/img/sc
reen-shot-2018-05-04-at-2.49.48-pm.png" alt="algo" title="ShowMyImage" /></p>

<p>Off - Policy Learning</p>

<blockquote>
  <p>The on-policy approach in the preceding section is actually a compromise‚Äîit
learns action values not for the optimal policy, but for a near-optimal policy
that still explores. A more
straightforward approach is to use two policies, one that is learned about and
that becomes the optimal
policy, and one that is more exploratory and is used to generate behavior. The
policy being learned
about is called the target policy, and the policy used to generate behavior is
called the behavior policy.
In this case we say that learning is from data ‚Äúoff‚Äù the target policy, and the
overall process is termed
off-policy learning.</p>
</blockquote>

<blockquote>
  <p>Off-policy methods require additional concepts
and notation, and because the data is due to a different policy, off-policy
methods are often of greater
variance and are slower to converge. On the other hand, off-policy methods are
more powerful and
general.</p>
</blockquote>

<blockquote>
  <p>Off-policy methods also have a variety of additional uses in applications. For
example,
they can often be applied to learn from data generated by a conventional non-
learning controller, or
from a human expert.</p>
</blockquote>
:ET